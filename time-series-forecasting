{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2fd39d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-05T13:57:59.846986Z",
     "iopub.status.busy": "2025-11-05T13:57:59.846757Z",
     "iopub.status.idle": "2025-11-05T13:58:08.078698Z",
     "shell.execute_reply": "2025-11-05T13:58:08.077915Z"
    },
    "papermill": {
     "duration": 8.237285,
     "end_time": "2025-11-05T13:58:08.080228",
     "exception": false,
     "start_time": "2025-11-05T13:57:59.842943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings, gc, os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b20daad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:58:08.085627Z",
     "iopub.status.busy": "2025-11-05T13:58:08.085147Z",
     "iopub.status.idle": "2025-11-05T13:58:08.089439Z",
     "shell.execute_reply": "2025-11-05T13:58:08.088787Z"
    },
    "papermill": {
     "duration": 0.008291,
     "end_time": "2025-11-05T13:58:08.090897",
     "exception": false,
     "start_time": "2025-11-05T13:58:08.082606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- Helpers ----------\n",
    "def rmsle(y_true, y_pred):\n",
    "    y_pred = np.maximum(0, y_pred)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175759c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:58:08.095922Z",
     "iopub.status.busy": "2025-11-05T13:58:08.095612Z",
     "iopub.status.idle": "2025-11-05T13:58:08.100438Z",
     "shell.execute_reply": "2025-11-05T13:58:08.099795Z"
    },
    "papermill": {
     "duration": 0.008552,
     "end_time": "2025-11-05T13:58:08.101695",
     "exception": false,
     "start_time": "2025-11-05T13:58:08.093143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- Enhanced Feature Engineering ----------\n",
    "def preprocess_holidays_enhanced(hol_df):\n",
    "    \"\"\"Better holiday processing\"\"\"\n",
    "    hol = hol_df.copy()\n",
    "    hol['date'] = pd.to_datetime(hol['date'])\n",
    "    hol['transferred'] = hol['transferred'].fillna(False)\n",
    "    \n",
    "    # Create holiday flags\n",
    "    valid_holidays = hol[(hol['transferred'] == False) & (hol['type'] != 'Work Day')]\n",
    "    \n",
    "    # National holidays\n",
    "    national_hols = valid_holidays[valid_holidays['locale'] == 'National'][['date']].drop_duplicates()\n",
    "    national_hols['is_holiday'] = 1\n",
    "    \n",
    "    return national_hols\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a97fadd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:58:08.106625Z",
     "iopub.status.busy": "2025-11-05T13:58:08.106399Z",
     "iopub.status.idle": "2025-11-05T13:58:08.112958Z",
     "shell.execute_reply": "2025-11-05T13:58:08.112427Z"
    },
    "papermill": {
     "duration": 0.010075,
     "end_time": "2025-11-05T13:58:08.114034",
     "exception": false,
     "start_time": "2025-11-05T13:58:08.103959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_enhanced_date_features(df):\n",
    "    \"\"\"More comprehensive date features\"\"\"\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Basic time features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    # Advanced time features\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_year_start'] = df['date'].dt.is_year_start.astype(int)\n",
    "    df['is_year_end'] = df['date'].dt.is_year_end.astype(int)\n",
    "    df['is_weekend'] = (df['date'].dt.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    # Payday effects\n",
    "    df['is_payday'] = ((df['day'] == 15) | (df['date'] == df['date'] + pd.offsets.MonthEnd(0))).astype(int)\n",
    "    \n",
    "    # Seasonal features\n",
    "    df['is_christmas'] = ((df['month'] == 12) & (df['day'] >= 15) & (df['day'] <= 31)).astype(int)\n",
    "    df['is_newyear'] = ((df['month'] == 1) & (df['day'] <= 7)).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a1c46d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:58:08.118494Z",
     "iopub.status.busy": "2025-11-05T13:58:08.118208Z",
     "iopub.status.idle": "2025-11-05T13:58:08.123083Z",
     "shell.execute_reply": "2025-11-05T13:58:08.121626Z"
    },
    "papermill": {
     "duration": 0.009162,
     "end_time": "2025-11-05T13:58:08.125047",
     "exception": false,
     "start_time": "2025-11-05T13:58:08.115885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_lag_features_for_train(df, group_cols, lag_days):\n",
    "    \"\"\"Create lag features only for training data\"\"\"\n",
    "    df = df.sort_values(group_cols + ['date']).copy()\n",
    "    \n",
    "    for lag in lag_days:\n",
    "        df[f'lag_{lag}'] = df.groupby(group_cols)['sales'].shift(lag)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c440802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:58:08.130476Z",
     "iopub.status.busy": "2025-11-05T13:58:08.130199Z",
     "iopub.status.idle": "2025-11-05T13:58:08.135698Z",
     "shell.execute_reply": "2025-11-05T13:58:08.134752Z"
    },
    "papermill": {
     "duration": 0.009565,
     "end_time": "2025-11-05T13:58:08.137145",
     "exception": false,
     "start_time": "2025-11-05T13:58:08.127580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_rolling_features_for_train(df, group_cols, windows):\n",
    "    \"\"\"Create rolling features only for training data\"\"\"\n",
    "    df = df.sort_values(group_cols + ['date']).copy()\n",
    "    \n",
    "    for window in windows:\n",
    "        df[f'roll_mean_{window}'] = df.groupby(group_cols)['sales']\\\n",
    "                                    .shift(1)\\\n",
    "                                    .rolling(window=window, min_periods=1)\\\n",
    "                                    .mean()\\\n",
    "                                    .reset_index(level=0, drop=True)\n",
    "        \n",
    "        df[f'roll_std_{window}'] = df.groupby(group_cols)['sales']\\\n",
    "                                   .shift(1)\\\n",
    "                                   .rolling(window=window, min_periods=1)\\\n",
    "                                   .std()\\\n",
    "                                   .reset_index(level=0, drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13807120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:58:08.142504Z",
     "iopub.status.busy": "2025-11-05T13:58:08.142159Z",
     "iopub.status.idle": "2025-11-05T13:58:08.149321Z",
     "shell.execute_reply": "2025-11-05T13:58:08.148650Z"
    },
    "papermill": {
     "duration": 0.01159,
     "end_time": "2025-11-05T13:58:08.150962",
     "exception": false,
     "start_time": "2025-11-05T13:58:08.139372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_test_features(train_df, test_df, group_cols, lag_days, windows):\n",
    "    \"\"\"Prepare test features using the last available values from training data\"\"\"\n",
    "    \n",
    "    # Get the last date in training data\n",
    "    last_train_date = train_df['date'].max()\n",
    "    \n",
    "    # Get sales values from the last available date for each store-family\n",
    "    last_sales = train_df[train_df['date'] == last_train_date][group_cols + ['sales']].copy()\n",
    "    last_sales = last_sales.rename(columns={'sales': 'last_sales'})\n",
    "    \n",
    "    # For store-families not present on the last date, get their most recent sales\n",
    "    all_combinations = test_df[group_cols].drop_duplicates()\n",
    "    missing_combinations = all_combinations.merge(\n",
    "        last_sales[group_cols], on=group_cols, how='left', indicator=True\n",
    "    )\n",
    "    missing_combinations = missing_combinations[missing_combinations['_merge'] == 'left_only'][group_cols]\n",
    "    \n",
    "    if len(missing_combinations) > 0:\n",
    "        # Get the most recent sales for missing combinations\n",
    "        recent_sales = train_df.merge(missing_combinations, on=group_cols)\\\n",
    "                              .sort_values(['date'])\\\n",
    "                              .groupby(group_cols)\\\n",
    "                              .last()\\\n",
    "                              .reset_index()[group_cols + ['sales']]\n",
    "        recent_sales = recent_sales.rename(columns={'sales': 'last_sales'})\n",
    "        \n",
    "        # Combine both\n",
    "        last_sales = pd.concat([last_sales, recent_sales], ignore_index=True)\n",
    "    \n",
    "    # Merge with test data\n",
    "    test_with_last = test_df.merge(last_sales, on=group_cols, how='left')\n",
    "    \n",
    "    # Fill any remaining missing values with 0\n",
    "    test_with_last['last_sales'] = test_with_last['last_sales'].fillna(0)\n",
    "    \n",
    "    # Create lag features using the last known value\n",
    "    for lag in lag_days:\n",
    "        test_with_last[f'lag_{lag}'] = test_with_last['last_sales']\n",
    "    \n",
    "    # Create rolling features using the last known value\n",
    "    for window in windows:\n",
    "        test_with_last[f'roll_mean_{window}'] = test_with_last['last_sales']\n",
    "        test_with_last[f'roll_std_{window}'] = 0  # We don't know std for test\n",
    "    \n",
    "    return test_with_last.drop('last_sales', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd4b63a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T13:58:08.157059Z",
     "iopub.status.busy": "2025-11-05T13:58:08.156711Z",
     "iopub.status.idle": "2025-11-05T14:03:05.832408Z",
     "shell.execute_reply": "2025-11-05T14:03:05.831332Z"
    },
    "papermill": {
     "duration": 297.680255,
     "end_time": "2025-11-05T14:03:05.833709",
     "exception": false,
     "start_time": "2025-11-05T13:58:08.153454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing...\n",
      "Creating base features...\n",
      "Creating lag/rolling features for training data...\n",
      "Preparing test features...\n",
      "Encoding categorical features...\n",
      "Creating aggregate features...\n",
      "Filling NaN values...\n",
      "Using 42 features\n",
      "Sample features: ['store_nbr', 'onpromotion', 'year', 'month', 'day', 'dayofweek', 'weekofyear', 'dayofyear', 'quarter', 'is_month_start']\n",
      "Training samples: 2972376, Validation samples: 28512\n",
      "Training model...\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.392262\tvalid_1's rmse: 0.404322\n",
      "[200]\ttraining's rmse: 0.376051\tvalid_1's rmse: 0.394138\n",
      "[300]\ttraining's rmse: 0.369985\tvalid_1's rmse: 0.390843\n",
      "[400]\ttraining's rmse: 0.365834\tvalid_1's rmse: 0.388334\n",
      "[500]\ttraining's rmse: 0.36259\tvalid_1's rmse: 0.386612\n",
      "[600]\ttraining's rmse: 0.359904\tvalid_1's rmse: 0.385607\n",
      "[700]\ttraining's rmse: 0.357555\tvalid_1's rmse: 0.384559\n",
      "[800]\ttraining's rmse: 0.355471\tvalid_1's rmse: 0.383792\n",
      "[900]\ttraining's rmse: 0.353647\tvalid_1's rmse: 0.382785\n",
      "[1000]\ttraining's rmse: 0.351871\tvalid_1's rmse: 0.382348\n",
      "[1100]\ttraining's rmse: 0.350238\tvalid_1's rmse: 0.381865\n",
      "[1200]\ttraining's rmse: 0.34868\tvalid_1's rmse: 0.381344\n",
      "[1300]\ttraining's rmse: 0.347126\tvalid_1's rmse: 0.381096\n",
      "[1400]\ttraining's rmse: 0.345714\tvalid_1's rmse: 0.380865\n",
      "[1500]\ttraining's rmse: 0.344411\tvalid_1's rmse: 0.380523\n",
      "[1600]\ttraining's rmse: 0.343007\tvalid_1's rmse: 0.380163\n",
      "[1700]\ttraining's rmse: 0.341634\tvalid_1's rmse: 0.379705\n",
      "[1800]\ttraining's rmse: 0.340513\tvalid_1's rmse: 0.379725\n",
      "[1900]\ttraining's rmse: 0.339202\tvalid_1's rmse: 0.379425\n",
      "[2000]\ttraining's rmse: 0.338067\tvalid_1's rmse: 0.37925\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1978]\ttraining's rmse: 0.338307\tvalid_1's rmse: 0.379232\n",
      "Validation RMSLE: 0.3792\n",
      "Submission saved!\n",
      "\n",
      "Top 20 features:\n",
      "            feature  importance\n",
      "21            lag_1       17395\n",
      "7         dayofyear       16593\n",
      "22            lag_7       14079\n",
      "4               day       12866\n",
      "23           lag_14       11786\n",
      "31          sf_mean       11527\n",
      "33           sf_std       11345\n",
      "24           lag_28       10589\n",
      "34           f_mean        9968\n",
      "0         store_nbr        9928\n",
      "38       family_enc        9100\n",
      "18              oil        8505\n",
      "36       store_mean        8211\n",
      "19        oil_lag_7        7883\n",
      "5         dayofweek        7690\n",
      "6        weekofyear        7621\n",
      "32        sf_median        7276\n",
      "20  oil_roll_mean_7        6741\n",
      "37        store_std        5865\n",
      "2              year        5822\n"
     ]
    }
   ],
   "source": [
    "# ---------- Fixed Main Pipeline ----------\n",
    "def main():\n",
    "    INPUT = '/kaggle/input/store-sales-time-series-forecasting'\n",
    "    print(\"Loading data...\")\n",
    "    train = pd.read_csv(os.path.join(INPUT, 'train.csv'))\n",
    "    test  = pd.read_csv(os.path.join(INPUT, 'test.csv'))\n",
    "    stores = pd.read_csv(os.path.join(INPUT, 'stores.csv'))\n",
    "    oil = pd.read_csv(os.path.join(INPUT, 'oil.csv'))\n",
    "    holidays = pd.read_csv(os.path.join(INPUT, 'holidays_events.csv'))\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    print(\"Preprocessing...\")\n",
    "    holiday_flags = preprocess_holidays_enhanced(holidays)\n",
    "    \n",
    "    # Oil features\n",
    "    oil['date'] = pd.to_datetime(oil['date'])\n",
    "    oil['oil'] = oil['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')\n",
    "    oil['oil_lag_7'] = oil['oil'].shift(7)\n",
    "    oil['oil_roll_mean_7'] = oil['oil'].rolling(7, min_periods=1).mean()\n",
    "    \n",
    "    # Merge store data\n",
    "    train = train.merge(stores, on='store_nbr', how='left')\n",
    "    test = test.merge(stores, on='store_nbr', how='left')\n",
    "    \n",
    "    # Create base features\n",
    "    print(\"Creating base features...\")\n",
    "    train_featured = create_enhanced_date_features(train)\n",
    "    test_featured = create_enhanced_date_features(test)\n",
    "    \n",
    "    # Add oil features\n",
    "    train_featured = train_featured.merge(oil, on='date', how='left')\n",
    "    test_featured = test_featured.merge(oil, on='date', how='left')\n",
    "    \n",
    "    # Add holiday features\n",
    "    train_featured = train_featured.merge(holiday_flags, on='date', how='left')\n",
    "    test_featured = test_featured.merge(holiday_flags, on='date', how='left')\n",
    "    train_featured['is_holiday'] = train_featured['is_holiday'].fillna(0)\n",
    "    test_featured['is_holiday'] = test_featured['is_holiday'].fillna(0)\n",
    "    \n",
    "    # Ensure onpromotion is filled\n",
    "    train_featured['onpromotion'] = train_featured['onpromotion'].fillna(0).astype(int)\n",
    "    test_featured['onpromotion'] = test_featured['onpromotion'].fillna(0).astype(int)\n",
    "    \n",
    "    # Create lag and rolling features ONLY for training data\n",
    "    print(\"Creating lag/rolling features for training data...\")\n",
    "    train_featured = create_lag_features_for_train(\n",
    "        train_featured, \n",
    "        ['store_nbr', 'family'], \n",
    "        [1, 7, 14, 28]\n",
    "    )\n",
    "    \n",
    "    train_featured = create_rolling_features_for_train(\n",
    "        train_featured,\n",
    "        ['store_nbr', 'family'],\n",
    "        [7, 14, 28]\n",
    "    )\n",
    "    \n",
    "    # Prepare test features using last known values from training\n",
    "    print(\"Preparing test features...\")\n",
    "    test_featured = prepare_test_features(\n",
    "        train_featured, \n",
    "        test_featured, \n",
    "        ['store_nbr', 'family'],\n",
    "        [1, 7, 14, 28],\n",
    "        [7, 14, 28]\n",
    "    )\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    print(\"Encoding categorical features...\")\n",
    "    cat_cols = ['family', 'city', 'state', 'type']\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on combined train+test for consistency\n",
    "        combined = pd.concat([train_featured[col], test_featured[col]], axis=0)\n",
    "        le.fit(combined)\n",
    "        train_featured[f'{col}_enc'] = le.transform(train_featured[col])\n",
    "        test_featured[f'{col}_enc'] = le.transform(test_featured[col])\n",
    "    \n",
    "    # Aggregate features from training data only\n",
    "    print(\"Creating aggregate features...\")\n",
    "    agg_features = train_featured.groupby(['store_nbr', 'family'])['sales'].agg([\n",
    "        'mean', 'median', 'std'\n",
    "    ]).reset_index()\n",
    "    agg_features.columns = ['store_nbr', 'family', 'sf_mean', 'sf_median', 'sf_std']\n",
    "    \n",
    "    train_featured = train_featured.merge(agg_features, on=['store_nbr', 'family'], how='left')\n",
    "    test_featured = test_featured.merge(agg_features, on=['store_nbr', 'family'], how='left')\n",
    "    \n",
    "    # Family-level aggregates\n",
    "    family_agg = train_featured.groupby('family')['sales'].agg([\n",
    "        'mean', 'std'\n",
    "    ]).reset_index()\n",
    "    family_agg.columns = ['family', 'f_mean', 'f_std']\n",
    "    \n",
    "    train_featured = train_featured.merge(family_agg, on='family', how='left')\n",
    "    test_featured = test_featured.merge(family_agg, on='family', how='left')\n",
    "    \n",
    "    # Store-level aggregates\n",
    "    store_agg = train_featured.groupby('store_nbr')['sales'].agg([\n",
    "        'mean', 'std'\n",
    "    ]).reset_index()\n",
    "    store_agg.columns = ['store_nbr', 'store_mean', 'store_std']\n",
    "    \n",
    "    train_featured = train_featured.merge(store_agg, on='store_nbr', how='left')\n",
    "    test_featured = test_featured.merge(store_agg, on='store_nbr', how='left')\n",
    "    \n",
    "    # Fill NaN values\n",
    "    print(\"Filling NaN values...\")\n",
    "    # For train: fill lag/rolling features with 0 or appropriate values\n",
    "    lag_roll_cols = [col for col in train_featured.columns if col.startswith('lag_') or col.startswith('roll_')]\n",
    "    for col in lag_roll_cols:\n",
    "        train_featured[col] = train_featured[col].fillna(0)\n",
    "    \n",
    "    # Fill other numeric columns\n",
    "    numeric_cols_train = train_featured.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols_test = test_featured.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    train_featured[numeric_cols_train] = train_featured[numeric_cols_train].fillna(0)\n",
    "    test_featured[numeric_cols_test] = test_featured[numeric_cols_test].fillna(0)\n",
    "    \n",
    "    # Define features (only those present in both train and test)\n",
    "    base_features = [\n",
    "        'store_nbr', 'onpromotion', 'year', 'month', 'day', 'dayofweek', \n",
    "        'weekofyear', 'dayofyear', 'quarter', 'is_month_start', 'is_month_end',\n",
    "        'is_quarter_start', 'is_quarter_end', 'is_weekend', 'is_payday',\n",
    "        'is_christmas', 'is_newyear', 'is_holiday',\n",
    "        'oil', 'oil_lag_7', 'oil_roll_mean_7'\n",
    "    ]\n",
    "    \n",
    "    lag_features = [f'lag_{l}' for l in [1, 7, 14, 28]]\n",
    "    roll_features = [f'roll_mean_{w}' for w in [7, 14, 28]] + [f'roll_std_{w}' for w in [7, 14, 28]]\n",
    "    agg_features = ['sf_mean', 'sf_median', 'sf_std', 'f_mean', 'f_std', 'store_mean', 'store_std']\n",
    "    enc_features = [f'{col}_enc' for col in cat_cols]\n",
    "    \n",
    "    # Only include features that exist in both datasets\n",
    "    all_features = []\n",
    "    for feature_list in [base_features, lag_features, roll_features, agg_features, enc_features]:\n",
    "        for feature in feature_list:\n",
    "            if feature in train_featured.columns and feature in test_featured.columns:\n",
    "                all_features.append(feature)\n",
    "    \n",
    "    print(f\"Using {len(all_features)} features\")\n",
    "    print(\"Sample features:\", all_features[:10])\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = train_featured[all_features]\n",
    "    y_train = np.log1p(train_featured['sales'])  # Log transformation\n",
    "    \n",
    "    # Time-based validation split\n",
    "    train_dates = pd.to_datetime(train_featured['date'])\n",
    "    last_date = train_dates.max()\n",
    "    val_start = last_date - pd.Timedelta(days=15)\n",
    "    \n",
    "    train_mask = train_dates < val_start\n",
    "    val_mask = train_dates >= val_start\n",
    "    \n",
    "    X_tr = X_train[train_mask]\n",
    "    y_tr = y_train[train_mask]\n",
    "    X_val = X_train[val_mask]\n",
    "    y_val = y_train[val_mask]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_tr)}, Validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # LightGBM parameters\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 128,\n",
    "        'max_depth': -1,\n",
    "        'min_data_in_leaf': 100,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'lambda_l1': 0.1,\n",
    "        'lambda_l2': 0.1,\n",
    "        'min_gain_to_split': 0.01,\n",
    "        'seed': 42,\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    lgb_train = lgb.Dataset(X_tr, y_tr)\n",
    "    lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(200),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_pred_log = model.predict(X_val)\n",
    "    val_pred = np.expm1(val_pred_log)\n",
    "    val_true = np.expm1(y_val)\n",
    "    \n",
    "    val_rmsle = rmsle(val_true, val_pred)\n",
    "    print(f\"Validation RMSLE: {val_rmsle:.4f}\")\n",
    "    \n",
    "    # Prepare test features\n",
    "    X_test = test_featured[all_features]\n",
    "    \n",
    "    # Predict\n",
    "    test_pred_log = model.predict(X_test)\n",
    "    test_pred = np.expm1(test_pred_log)\n",
    "    test_pred = np.clip(test_pred, 0, None)\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test['id'],\n",
    "        'sales': test_pred\n",
    "    })\n",
    "    \n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission saved!\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': all_features,\n",
    "        'importance': model.feature_importance()\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 20 features:\")\n",
    "    print(importance_df.head(20))\n",
    "    \n",
    "    return val_rmsle\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 2887556,
     "sourceId": 29781,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 311.001456,
   "end_time": "2025-11-05T14:03:06.858156",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-05T13:57:55.856700",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
