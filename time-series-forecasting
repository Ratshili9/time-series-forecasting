{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings, gc, os\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:45:13.270112Z","iopub.execute_input":"2025-11-04T15:45:13.270551Z","iopub.status.idle":"2025-11-04T15:45:13.276396Z","shell.execute_reply.started":"2025-11-04T15:45:13.270436Z","shell.execute_reply":"2025-11-04T15:45:13.275095Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# ---------- Helpers ----------\ndef rmsle(y_true, y_pred):\n    y_pred = np.maximum(0, y_pred)\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:45:13.277897Z","iopub.execute_input":"2025-11-04T15:45:13.278224Z","iopub.status.idle":"2025-11-04T15:45:13.295031Z","shell.execute_reply.started":"2025-11-04T15:45:13.278201Z","shell.execute_reply":"2025-11-04T15:45:13.293866Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    # basic downcast\n    for col in df.select_dtypes(include=['int64']).columns:\n        df[col] = pd.to_numeric(df[col], downcast='integer')\n    for col in df.select_dtypes(include=['float64']).columns:\n        df[col] = pd.to_numeric(df[col], downcast='float')\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:45:13.296639Z","iopub.execute_input":"2025-11-04T15:45:13.297133Z","iopub.status.idle":"2025-11-04T15:45:13.313295Z","shell.execute_reply.started":"2025-11-04T15:45:13.297085Z","shell.execute_reply":"2025-11-04T15:45:13.312258Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# ---------- Feature engineering ----------\ndef preprocess_holidays(hol_df):\n    hol = hol_df.copy()\n    hol['date'] = pd.to_datetime(hol['date'])\n    # transferred can be NaN -> treat as False\n    if 'transferred' in hol.columns:\n        hol['transferred'] = hol['transferred'].fillna(False)\n    else:\n        hol['transferred'] = False\n    # Only treat non-transferred, non-Work Day holidays as holidays\n    valid = hol[(hol['transferred'] == False) & (hol['type'] != 'Work Day')]\n    # Use the date as holiday flag; we keep type/locale if needed later\n    return valid[['date','type','locale']].drop_duplicates()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:45:13.314633Z","iopub.execute_input":"2025-11-04T15:45:13.314934Z","iopub.status.idle":"2025-11-04T15:45:13.335494Z","shell.execute_reply.started":"2025-11-04T15:45:13.314904Z","shell.execute_reply":"2025-11-04T15:45:13.334343Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def create_date_features(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df['year'] = df['date'].dt.year.astype(np.int16)\n    df['month'] = df['date'].dt.month.astype(np.int8)\n    df['day'] = df['date'].dt.day.astype(np.int8)\n    df['dayofweek'] = df['date'].dt.dayofweek.astype(np.int8)\n    # isocalendar().week returns UInt32 on some pandas; convert to int\n    df['weekofyear'] = df['date'].dt.isocalendar().week.astype(int)\n    df['dayofyear'] = df['date'].dt.dayofyear.astype(np.int16)\n    df['quarter'] = df['date'].dt.quarter.astype(np.int8)\n    df['is_month_end'] = df['date'].dt.is_month_end.astype(np.int8)\n    df['is_weekend'] = (df['dayofweek'] >= 5).astype(np.int8)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:45:13.337117Z","iopub.execute_input":"2025-11-04T15:45:13.337448Z","iopub.status.idle":"2025-11-04T15:45:13.358817Z","shell.execute_reply.started":"2025-11-04T15:45:13.337426Z","shell.execute_reply":"2025-11-04T15:45:13.357729Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def create_lag_rolling(df_all, group_cols=['store_nbr','family'], lag_days=[1,7,14,28], windows=[7,14,28]):\n    # df_all must be sorted by group_cols + date, and contain train then test rows (sales NaN in test)\n    grp_key = df_all[group_cols].astype(str).agg('_'.join, axis=1)\n    df_all['store_family'] = grp_key\n\n    # create lags\n    for lag in lag_days:\n        df_all[f'lag_{lag}'] = df_all.groupby('store_family')['sales'].shift(lag)\n\n    # rolling means/std calculated using previous values -> shift(1) before rolling\n    for w in windows:\n        df_all[f'roll_mean_{w}'] = df_all.groupby('store_family')['sales'].shift(1).rolling(window=w, min_periods=1).mean().reset_index(level=0, drop=True)\n        df_all[f'roll_std_{w}'] = df_all.groupby('store_family')['sales'].shift(1).rolling(window=w, min_periods=1).std().reset_index(level=0, drop=True)\n\n    # Fill rolling std NaN with 0\n    roll_cols = [c for c in df_all.columns if c.startswith('roll_')]\n    df_all[roll_cols] = df_all[roll_cols].fillna(0)\n\n    # If lag values at test horizon are NaN (shouldn't be if train immediately precedes test), fill with group-level stats\n    lag_cols = [f'lag_{l}' for l in lag_days]\n    # group-level last known sale\n    last_known = df_all[df_all['sales'].notna()].groupby('store_family')['sales'].last().rename('last_sale')\n    df_all = df_all.merge(last_known, on='store_family', how='left')\n    for c in lag_cols:\n        df_all[c] = df_all[c].fillna(df_all['last_sale'])\n    df_all = df_all.drop(columns=['last_sale'])\n    return df_all","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:45:13.359925Z","iopub.execute_input":"2025-11-04T15:45:13.360276Z","iopub.status.idle":"2025-11-04T15:45:13.378750Z","shell.execute_reply.started":"2025-11-04T15:45:13.360248Z","shell.execute_reply":"2025-11-04T15:45:13.377739Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# ---------- Main pipeline ----------\ndef main():\n    INPUT = '/kaggle/input/store-sales-time-series-forecasting'  # change if running locally\n    print(\"Loading data...\")\n    train = pd.read_csv(os.path.join(INPUT, 'train.csv'), parse_dates=['date'])\n    test  = pd.read_csv(os.path.join(INPUT, 'test.csv'), parse_dates=['date'])\n    stores = pd.read_csv(os.path.join(INPUT, 'stores.csv'))\n    oil = pd.read_csv(os.path.join(INPUT, 'oil.csv'))\n    holidays = pd.read_csv(os.path.join(INPUT, 'holidays_events.csv'))\n    sample_sub = pd.read_csv(os.path.join(INPUT, 'sample_submission.csv'))\n\n    # Basic cleaning\n    print(\"Preprocessing holidays and oil...\")\n    hol = preprocess_holidays(holidays)\n    oil['date'] = pd.to_datetime(oil['date'])\n    if 'dcoilwtico' in oil.columns:\n        oil = oil[['date','dcoilwtico']].rename(columns={'dcoilwtico':'oil'})\n    else:\n        oil = oil[['date']].rename(columns={'date':'date'})\n        oil['oil'] = np.nan\n    # forward/backfill oil\n    oil['oil'] = oil['oil'].fillna(method='ffill').fillna(method='bfill')\n\n    # Merge small tables into train and test for convenience (store meta)\n    train = train.merge(stores, on='store_nbr', how='left')\n    test  = test.merge(stores, on='store_nbr', how='left')\n\n    # Add oil and holiday presence later via concat strategy\n\n    # Add sales column to test (NaN) then concat so that groupby.shift works across the boundary\n    test['sales'] = np.nan\n    # preserve id for submission ordering\n    test_ids = test['id'].copy()\n\n    print(\"Concatenating train+test and creating date features...\")\n    df_all = pd.concat([train, test], sort=False).reset_index(drop=True)\n    df_all = create_date_features(df_all)\n\n    # Merge oil and holiday flag\n    df_all = df_all.merge(oil, on='date', how='left')\n    df_all['oil'] = df_all['oil'].fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n\n    df_all = df_all.merge(hol[['date']].assign(is_holiday=1), on='date', how='left')\n    df_all['is_holiday'] = df_all['is_holiday'].fillna(0).astype(np.int8)\n\n    # Ensure onpromotion present and numeric\n    if 'onpromotion' in df_all.columns:\n        df_all['onpromotion'] = df_all['onpromotion'].fillna(0).astype(np.int8)\n    else:\n        df_all['onpromotion'] = 0\n\n    # Create store_family key for grouping\n    df_all['store_nbr'] = df_all['store_nbr'].astype(int)\n\n    # Ensure correct ordering: by store_family then date\n    df_all = df_all.sort_values(['store_nbr','family','date']).reset_index(drop=True)\n\n    # Create lag and rolling features\n    print(\"Creating lag and rolling features...\")\n    df_all = create_lag_rolling(df_all, group_cols=['store_nbr','family'], lag_days=[1,7,14,28], windows=[7,14,28])\n\n    # Basic aggregated stats from training part\n    print(\"Creating aggregate features...\")\n    train_mask = df_all['sales'].notna()\n    agg_sf = df_all[train_mask].groupby(['store_nbr','family'])['sales'].agg(['mean','median','std']).reset_index().rename(columns={'mean':'sf_mean','median':'sf_median','std':'sf_std'})\n    df_all = df_all.merge(agg_sf, on=['store_nbr','family'], how='left')\n\n    agg_f = df_all[train_mask].groupby('family')['sales'].mean().reset_index().rename(columns={'sales':'f_mean'})\n    df_all = df_all.merge(agg_f, on='family', how='left')\n\n    # Label encode categorical variables on combined data to keep consistency\n    print(\"Encoding categorical features...\")\n    cat_cols = ['family','city','state','type']\n    encoders = {}\n    for c in cat_cols:\n        df_all[c] = df_all[c].astype(str).fillna('NA')\n        le = LabelEncoder()\n        df_all[c + '_enc'] = le.fit_transform(df_all[c])\n        encoders[c] = le\n\n    # Fill NaNs for aggregations\n    df_all['sf_mean'] = df_all['sf_mean'].fillna(0)\n    df_all['sf_median'] = df_all['sf_median'].fillna(0)\n    df_all['sf_std'] = df_all['sf_std'].fillna(0)\n    df_all['f_mean'] = df_all['f_mean'].fillna(0)\n\n    # Cast types to save memory\n    df_all = reduce_mem_usage(df_all)\n\n    # Split back into train and test\n    train_proc = df_all[df_all['sales'].notna()].copy()\n    test_proc  = df_all[df_all['sales'].isna()].copy()\n\n    # Target transformation\n    train_proc['y'] = np.log1p(train_proc['sales'].astype(float))\n\n    # Features to use\n    features = [\n        'store_nbr','onpromotion','year','month','day','dayofweek','dayofyear','weekofyear','quarter',\n        'is_month_end','is_weekend','oil','is_holiday',\n        'family_enc','city_enc','state_enc','type_enc',\n        'sf_mean','sf_median','sf_std','f_mean'\n    ]\n    # add lag and roll columns\n    lag_roll_cols = [c for c in train_proc.columns if c.startswith('lag_') or c.startswith('roll_')]\n    features += lag_roll_cols\n\n    # Ensure all features exist in test\n    features = [f for f in features if f in train_proc.columns and f in test_proc.columns]\n\n    print(f\"Using {len(features)} features.\")\n\n    # Train/validation split: last 15 days as validation\n    last_date = train_proc['date'].max()\n    val_start = last_date - pd.Timedelta(days=15)\n    train_mask_time = train_proc['date'] < val_start\n    val_mask_time   = train_proc['date'] >= val_start\n\n    X_train = train_proc.loc[train_mask_time, features]\n    y_train = train_proc.loc[train_mask_time, 'y']\n    X_val   = train_proc.loc[val_mask_time, features]\n    y_val   = train_proc.loc[val_mask_time, 'y']\n\n    print(\"Training LightGBM on log1p(sales)...\")\n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n\n    params = {\n        'objective':'regression',\n        'metric':'rmse',\n        'boosting_type':'gbdt',\n        'learning_rate':0.05,\n        'num_leaves':64,\n        'min_data_in_leaf':50,\n        'feature_fraction':0.8,\n        'bagging_fraction':0.8,\n        'bagging_freq':5,\n        'lambda_l1':0.1,\n        'lambda_l2':0.1,\n        'seed':42,\n        'verbosity':-1\n    }\n\n    model = lgb.train(\n        params,\n        lgb_train,\n        num_boost_round=2000,\n        valid_sets=[lgb_train, lgb_val],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=100),\n            lgb.log_evaluation(period=100)\n        ]\n    )\n\n\n    # Validate\n    val_pred_log = model.predict(X_val, num_iteration=model.best_iteration)\n    val_pred = np.expm1(val_pred_log)\n    y_val_true = np.expm1(y_val.values)\n    print(\"Validation RMSLE:\", rmsle(y_val_true, val_pred))\n\n    # Prepare test features\n    X_test = test_proc[features].copy()\n\n    # Predict (single model)\n    print(\"Predicting test set...\")\n    pred_log = model.predict(X_test, num_iteration=model.best_iteration)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0, None)\n\n    # Build submission aligned to test.id order\n    submission = pd.DataFrame({\n        'id': test_proc['id'].values,\n        'sales': pred\n    })\n    # Ensure order same as original test file\n    submission = submission.set_index('id').reindex(test_ids).reset_index()\n    submission.columns = ['id','sales']\n\n    submission.to_csv('submission.csv', index=False)\n    print(\" submission.csv\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:45:13.380000Z","iopub.execute_input":"2025-11-04T15:45:13.380301Z","iopub.status.idle":"2025-11-04T15:51:49.922959Z","shell.execute_reply.started":"2025-11-04T15:45:13.380269Z","shell.execute_reply":"2025-11-04T15:51:49.921745Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nPreprocessing holidays and oil...\nConcatenating train+test and creating date features...\nCreating lag and rolling features...\nCreating aggregate features...\nEncoding categorical features...\nUsing 31 features.\nTraining LightGBM on log1p(sales)...\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's rmse: 0.39261\tvalid_1's rmse: 0.397268\n[200]\ttraining's rmse: 0.376226\tvalid_1's rmse: 0.390264\n[300]\ttraining's rmse: 0.369835\tvalid_1's rmse: 0.386309\n[400]\ttraining's rmse: 0.365803\tvalid_1's rmse: 0.383955\n[500]\ttraining's rmse: 0.362937\tvalid_1's rmse: 0.382125\n[600]\ttraining's rmse: 0.360368\tvalid_1's rmse: 0.380576\n[700]\ttraining's rmse: 0.358096\tvalid_1's rmse: 0.379365\n[800]\ttraining's rmse: 0.356088\tvalid_1's rmse: 0.378421\n[900]\ttraining's rmse: 0.35434\tvalid_1's rmse: 0.377733\n[1000]\ttraining's rmse: 0.352673\tvalid_1's rmse: 0.376643\n[1100]\ttraining's rmse: 0.35123\tvalid_1's rmse: 0.375814\n[1200]\ttraining's rmse: 0.349891\tvalid_1's rmse: 0.375207\n[1300]\ttraining's rmse: 0.348607\tvalid_1's rmse: 0.374471\n[1400]\ttraining's rmse: 0.34751\tvalid_1's rmse: 0.374127\n[1500]\ttraining's rmse: 0.346387\tvalid_1's rmse: 0.373903\n[1600]\ttraining's rmse: 0.345363\tvalid_1's rmse: 0.373669\n[1700]\ttraining's rmse: 0.344363\tvalid_1's rmse: 0.373511\n[1800]\ttraining's rmse: 0.343397\tvalid_1's rmse: 0.373227\n[1900]\ttraining's rmse: 0.342533\tvalid_1's rmse: 0.373038\n[2000]\ttraining's rmse: 0.341684\tvalid_1's rmse: 0.37277\nDid not meet early stopping. Best iteration is:\n[2000]\ttraining's rmse: 0.341684\tvalid_1's rmse: 0.37277\nValidation RMSLE: 0.37276672838148817\nPredicting test set...\nWrote submission.csv\n","output_type":"stream"}],"execution_count":52}]}